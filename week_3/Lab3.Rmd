---
title: "Park_Lab3"
author: "Patty Park"
date: 
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(rsample)
library(glmnet)
```

## Lab 3: Predicting the age of abalone

Abalones are marine snails. Their flesh is widely considered to be a desirable food, and is consumed raw or cooked by a variety of cultures. The age of abalone is determined by cutting the shell through the cone, staining it, and counting the number of rings through a microscope -- a boring and time-consuming task. Other measurements, which are easier to obtain, are used to predict the age.

The data set provided includes variables related to the sex, physical dimensions of the shell, and various weight measurements, along with the number of rings in the shell. Number of rings is the stand-in here for age.

### Data Exploration

Pull the abalone data from Github and take a look at it.

```{r data, include=FALSE}
#read in data
abdat<- read_csv(file = "https://raw.githubusercontent.com/MaRo406/eds-232-machine-learning/main/data/abalone-data.csv")
#look at data
glimpse(abdat)
set.seed(123) #for reproducibility

```

### Data Splitting

-   ***Question 1***. Split the data into training and test sets. Use a 70/30 training/test split.

```{r}
#split data up
split <- initial_split(abdat, prop = 0.7)

#assign which parts of the data is for training and test
ab_train <-  training(split)
ab_test  <- testing(split)
```

We'll follow our text book's lead and use the caret package in our approach to this task. We will use the glmnet package in order to perform ridge regression and the lasso. The main function in this package is glmnet(), which can be used to fit ridge regression models, lasso models, and more. In particular, we must pass in an x matrix of predictors as well as a y outcome vector , and we do not use the yâˆ¼x syntax.

### Fit a ridge regression model

-   ***Question 2***. Use the model.matrix() function to create a predictor matrix, x, and assign the Rings variable to an outcome vector, y.

```{r}
# create model matrix on Rings and assign it on X
X <- model.matrix(Rings ~ .,ab_train)[,-1]

# assign Rings vector to Y
Y <- ab_train$Rings

```


-   ***Question 3***. Fit a ridge model (controlled by the alpha parameter) using the glmnet() function. Make a plot showing how the estimated coefficients change with lambda. (Hint: You can call plot() directly on the glmnet() objects).

```{r}
#create a ridge model
ridge <- glmnet(
  x = X,
  y = Y,
  alpha = 0
)

#create plot of ridge model
plot(ridge, xvar = "lambda")

#we can see how each estimated coefficient changes with lambda with coefficients starting beyond -5 to beyond 10. At the 6 lambda portion, they all end up at coefficient zero.
```


### Using *k*-fold cross validation resampling and tuning our models

In lecture we learned about two methods of estimating our model's generalization error by resampling, cross validation and bootstrapping. We'll use the *k*-fold cross validation method in this lab. Recall that lambda is a tuning parameter that helps keep our model from over-fitting to the training data. Tuning is the process of finding the optima value of lamba.

-   ***Question 4***. This time fit a ridge regression model and a lasso model, both with using cross validation. The glmnet package kindly provides a cv.glmnet() function to do this (similar to the glmnet() function that we just used). Use the alpha argument to control which type of model you are running. Plot the results.

```{r}
# Apply CV ridge regression to 
ridge <- cv.glmnet(
  x = X,
  y = Y,
  alpha = 0
)

# Apply CV lasso regression to abolone data
lasso <- cv.glmnet(
  x = X,
  y = Y,
  alpha = 1
)

#plot results
par(mfrow = c(1, 2)) #put plots next to each other side by side
plot(ridge, main = "Ridge penalty\n\n") #plot ridge results
plot(lasso, main = "Lasso penalty\n\n") #plot lasso results
```


-   ***Question 5***. Interpret the graphs. What is being displayed on the axes here? How does the performance of the models change with the value of lambda?

- **Answer**: In both the ridge and lasso graph, we see that on the x axis, the lambda is being displayed, giving us all the lambda values. On the y axis, we see that it is displaying the Mean-Squared Error. 

For the ridge graph, we see the red dots lining up in a somewhat smooth matter. I would say that the performance of the ridge model increases the MSE as the lambda increases, indicating to me that the model does not hold up very well. Between the first dotted line to the second dotted line, the red dots go up in slope, meaning that the Mean-Squared Errors have gone up. This would indicate that the model does not hold up too well as the lambda increases.

In the lasso plot, concerning performance, the model is performing well, and is slightly doing better than a regular OLS model. Between the two dotted lines, with the first dotted line indicating the lambda with a minimum MSE and the second dotted line indicating the largest lambda within one standard error, the MSE seems to stay the same, with it slightly increasing towards the second dotted line. Again, we see the MSE increasing as the lambda gets larger.

-   ***Question 6***. Inspect the ridge model object you created with cv.glmnet(). The \$cvm column shows the MSEs for each CV fold. What is the minimum MSE? What is the value of lambda associated with this MSE minimum?

```{r}
# Ridge model
# minimum MSE
ab_ridge_mse <- min(ridge$cvm)
print(ab_ridge_mse)

# lambda for this min MSE
ab_ridge_lambda_mse <- ridge$lambda.min
print(ab_ridge_lambda_mse)
```

- **Answer**: Here, our MSE for each CV fold in our lobster data is `r paste(round(ab_ridge_mse, 3))`. The value of lambda associated with the MSE minimum is `r paste(round(ab_ridge_lambda_mse, 3))`.

-   ***Question 7***. Do the same for the lasso model. What is the minimum MSE? What is the value of lambda associated with this MSE minimum?

```{r}
# Lasso model
# minimum MSE
ab_lasso_mse <- min(lasso$cvm)
print(ab_lasso_mse)

# lambda for this min MSE
ab_lasso_lambda_mse <- lasso$lambda.min
print(ab_lasso_lambda_mse)
```

- **Answer**: In our Lasso model, we get `r paste(round(ab_lasso_mse))`. The values associated with these MSE minimum is `r paste(round(ab_lasso_lambda_mse))`.

Data scientists often use the "one-standard-error" rule when tuning lambda to select the best model. This rule tells us to pick the most parsimonious model (fewest number of predictors) while still remaining within one standard error of the overall minimum cross validation error. The cv.glmnet() model object has a column that automatically finds the value of lambda associated with the model that produces an MSE that is one standard error from the MSE minimum (\$lambda.1se).


-   ***Question 8.*** Find the number of predictors associated with this model (hint: the \$nzero is the \# of predictors column).
```{r}
#what is this asking? is it asking for just lasso model?

#ridge model for number of predictors
predictors_ridge <- ridge$nzero[ridge$lambda == ridge$lambda.min]
print(predictors_ridge)
#ridge does not do anything. We just get the same number of features for the number of predictors as that is what the ridge model does

#lasso model for number of predictors
predictors_lasso <- lasso$nzero[lasso$lambda == lasso$lambda.min]
print(predictors_lasso)
```


-   ***Question 9*** Which regularized regression worked better for this task, ridge or lasso? Explain your answer.

- **Answer**: Between the two models, lasso is the better model for a number of reasons:

- The lasso penalty chart shows that MSE stays relatively the same within one standard error from the minimum lambda
- The lasso minimum MSE is also smaller than the ridge minimum MSE
- In the number of predictors, the lasso model is at s80 (iteration), an earlier iteration while the ridge model is at s99 (iteration).



