---
title: "Lab6"
author: Patty Park
date: "2023-03-01"
output: html_document
---

We are using the variables to predict presense or absence of the species (eel)

note on mtry:needs to be given the full set of features to see what it needs to train on




## Case Study: Eel Distribution Modeling

This week's lab follows a project modeling the eel species Anguilla australis described by Elith et al. (2008). There are two data sets for this lab.  You'll use one for training and evaluating your model, and you'll use your model to make predictions predictions on the other.  Then you'll compare your model's performance to the model used by Elith et al.

```{r}
#load libraries
library(tidyverse)
library(tidymodels)
library(xgboost)
library(janitor)
```


## Data

Grab the training data sets (eel.model.data.csv, eel.eval.data.csv) from github here:
https://github.com/MaRo406/eds-232-machine-learning/blob/main/data 

```{r}
eel_model <- read_csv(here::here("week_7", "data", "eel.model.data.csv")) %>% 
  clean_names() %>% 
  select(-site) %>% 
  mutate(angaus = as.factor(angaus),
         method = as.factor(method))
  #ASK ALLIE WHY WE CHANGE THESE INTO FACTOR##
eel_eval <- read_csv(here::here("week_7", "data", "eel.eval.data.csv"))
```


### Split and Resample

Split the model data (eel.model.data.csv) into a training and test set, stratified by outcome score (Angaus). Use 10-fold CV to resample the training set.

```{r}
#set seed for reproducibility
set.seed(50)

#split data into 80 20 split
eel_split <- initial_split(eel_model, prop = .8, strata = angaus) #split data stratified by survived

eel_train <- training(eel_split)#get training data
eel_test = testing(eel_split) #get testing data

#look at first 5 per training and testing data
head(eel_train)
head(eel_test)

#set folds to 5
cv_folds = vfold_cv(eel_train, v = 5)
```


### Preprocess

Create a recipe to prepare your data for the XGBoost model

```{r}
#create eel recipe
eel_recipe <- recipe(angaus ~ ., data = eel_train) %>% #create model recipe
  step_dummy(all_nominal_predictors()) %>% #create dummy variables from all factors
  step_normalize(all_numeric_predictors()) #normalize all numeric predictors
```


## Tuning XGBoost

### Tune Learning Rate

Following the XGBoost tuning strategy outlined in lecture, first we conduct tuning on just the learning rate parameter:

1.  Create a model specification using {xgboost} for the estimation

-   Only specify one parameter to tune()

```{r}
#create model
eel_xgb_model <- boost_tree(learn_rate = tune()) %>% #tuning the learn_rate and trees for the parameter
  set_engine("xgboost") %>%  #nthread = 2
  set_mode("classification")

#create workflow
eel_xgb_workflow = workflow() %>% #create workflow
  add_model(eel_xgb_model) %>% #add boosted trees model
  add_recipe(eel_recipe) #add recipe

eel_xgb_workflow
```

2.  Set up a grid to tune your model by using a range of learning rate parameter values: expand.grid(learn_rate = seq(0.0001, 0.3, length.out = 30))

-   Use appropriate metrics argument(s) - Computational efficiency becomes a factor as models get more complex and data get larger. Record the time it takes to run. Do this for each tuning phase you run.You could use {tictoc} or system.time().

Note:
- defining the grid: matrix of parameter values (use on difference cv folds) see what is the best set of values
- giving a set of value to define

```{r}
#tune the model using created grid
# system.time(
#   eel_xbg_tune <- eel_xgb_workflow %>% 
#     tune_grid(resamples = cv_folds, grid = 10) %>% 
#     expand.grid(learn_rate = seq(0.0001, 0.3, length.out = 30))
# )


###ASK ALLIE QUESTION
grid_1 <- expand.grid(learn_rate = seq(0.0001, 0.3, length.out = 30))

#tune the model using created grid
system.time(
  eel_xbg_tune <- eel_xgb_workflow %>%
    tune_grid(resamples = cv_folds, grid = grid_1)
)

save(eel_xbg_tune, file = "eel_xbg_tune.rda")

load(file = here::here("week_7",  "eel_xbg_tune.rda"))

eel_xbg_tune
```
 The time it took for this to run was about 11.7 seconds

3.  Show the performance of the best models and the estimates for the learning rate parameter values associated with each.

```{r}
#look at performance of model
autoplot(eel_xbg_tune)

tree_learn <- show_best(eel_xbg_tune, n = 1)

tree_learn




# #finalize work flow for both metrics
# rf_final_acc <- finalize_workflow(eel_xgb_workflow, select_best(eel_xbg_tune_2, metric = "accuracy"))
# 
# rf_final_roc <- finalize_workflow(eel_xgb_workflow, select_best(eel_xbg_tune_2, metric = "roc_auc"))

# #print out results
# rf_final_acc
# rf_final_roc
# 
# 
# #fit finalized workflow with training data
# fit_final_acc <- fit(rf_final_acc, eel_train)
# fit_final_roc <- fit(rf_final_roc, eel_train)
# 
# #print fitted results
# fit_final_acc
# fit_final_roc
# 
# #
# train_predict_acc <- predict(object = fit_final_acc, new_data = eel_train) %>% # predict the training set
#   bind_cols(eel_train)
# train_predict_acc 
# 
# train_predict_roc <- predict(object = fit_final_acc, new_data = eel_train) %>% # predict the training set
#   bind_cols(eel_train)
# 
# train_metrics2 <- train_predict_acc %>%
#   metrics(percent, .pred_class) # get testing data metrics
# 
# train_metrics2 <- train_predict_roc %>%
#   metrics(percent, .pred) # get testing data metrics



```

Looks like roc_auc curve model is better, uses less trees and has larger metric

### Tune Tree Parameters

1.  Create a new specification where you set the learning rate (which you already optimized) and tune the tree parameters.

```{r}
learn_rate <- tree_learn$learn_rate

#create model setting the learning rate
eel_xgb_model_learn <- boost_tree(learn_rate = learn_rate, trees = 3000, tree_depth = tune(), min_n = tune(), loss_reduction = tune()) %>% #tuning the learn_rate and trees for the parameter
  set_engine("xgboost") %>%  #nthread = 2
  set_mode("classification")

#create the workflow
eel_xgb_workflow_learn <- workflow() %>% #create workflow
  add_model(eel_xgb_model_learn) %>% #add boosted trees model
  add_recipe(eel_recipe)

eel_xgb_workflow_learn
```

2.  Set up a tuning grid. This time use grid_latin_hypercube() to get a representative sampling of the parameter space

```{r}
#set up tuning grid using the `grid_latin_hypercube()`

grid_2 <- grid_latin_hypercube(tree_depth(), min_n(), loss_reduction())


system.time(
  eel_xbg_tune_latin <- eel_xgb_workflow_learn %>%
    tune_grid(resamples = cv_folds, grid = grid_2)
)

eel_xbg_tune_latin

```

time is 24.28 seconds


3.  Show the performance of the best models and the estimates for the tree parameter values associated with each.

```{r}
#graph the performance of the tuned model
autoplot(eel_xbg_tune_latin)

tree_learn_2 <- show_best(eel_xbg_tune_latin, n = 1)

tree_learn_2

min_n <- tree_learn_2$min_n
tree_depth <- tree_learn_2$tree_depth
loss_reduction <- tree_learn_2$loss_reduction
```


### Tune Stochastic Parameters

1.  Create a new specification where you set the learning rate and tree parameters (which you already optimized) and tune the stochastic parameters.

```{r}
#create model
eel_xgb_model_stoch <- boost_tree(learn_rate = learn_rate, trees = 3000,
                                  tree_depth = tree_depth,
                                  min_n = min_n,
                                  loss_reduction = loss_reduction,
                                  mtry = tune(),
                                  sample_size = tune()) %>% #tuning the learn_rate and trees for the parameter
  set_engine("xgboost") %>%  #nthread = 2
  set_mode("classification")

#create workflow
eel_xgb_workflow_stoch <- workflow() %>% #create workflow
  add_model(eel_xgb_model_stoch) %>% #add boosted trees model
  add_recipe(eel_recipe) #add recipe

eel_xgb_workflow_stoch



#tuning is not working for this part when setting everything in we dont know why. problem is sample size
```


2.  Set up a tuning grid. Use grid_latin_hypercube() again.


```{r}
#create grid for sample_prop() and mtry()
grid_3 <- grid_latin_hypercube(
  sample_size = sample_prop(),
  finalize(mtry(), eel_train)
)

# how you can set range for any of the parameters. ex can look at certain range for range.
# sample_size() %>% range_set()

# tune grid from the workflow
system.time(
  eel_xbg_tune_stoch <- eel_xgb_workflow_stoch %>%
    tune_grid(resamples = cv_folds, grid = grid_3)
)

# look at tuned grid
eel_xbg_tune_stoch
```

time it took 19.32


3.  Show the performance of the best models and the estimates for the tree parameter values associated with each.

```{r}
#
autoplot(eel_xbg_tune_stoch)

#
tree_learn_3 <- show_best(eel_xbg_tune_stoch, n = 1)

#
tree_learn_3

```


## Finalize workflow and make final prediction

1.  How well did your model perform? What types of errors did it make?
```{r}

```


## Fit your model the evaluation data and compare performance

1.  Now used your final model to predict on the other dataset (eval.data.csv)
```{r}

```


2.  How does your model perform on this data?

3.  How do your results compare to those of Elith et al.?

-   Use {vip} to compare variable importance
-   What do your variable importance results tell you about the distribution of this eel species?
