---
title: "Lab5_Demo"
author: "Mateo Robbins"
date: "2024-02-07"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)    
library(ggplot2) #great plots
library(rsample)  #data splitting 
library(recipes) #data preprocessing
library(skimr) #data exploration
library(tidymodels) #re-entering tidymodel mode
library(kknn) #knn modeling
```

###k-nearest neighbor in tidymodels

## Data

```{r data}
data(attrition)
churn <- attrition %>% mutate_if(is.ordered, .funs = factor, ordered = F) 
#skim(churn) run in console
```

Not doing the data exploration here in the interest of time and since we are familiar with this dataset.

```{r initial_split}
set.seed(808)
#initial split of data, default 75/25
churn_split <- initial_split(churn)
churn_test<- testing(churn_split)
churn_train <- training(churn_split)
```

We need to create a recipe and do the preprocessing by dummy coding the nominal variables and standardizing the numeric variables.


```{r recipe}
#preprocessing
knn_rec <- recipe(Attrition ~., data = churn_train) %>% 
  step_dummy(all_nominal(), -all_outcomes(), one_hot = T) %>% 
  step_normalize(all_numeric(), - all_outcomes (),) %>% #normalize the distance to get to the same scale cause its sensitive (centering and scaling: converting mean to zero and standarizing the SD)
  prep()

#mean and SD is done with the training data
baked_train <- bake(knn_rec, churn_train)
```

Recall: if you want to see the what the recipe is doing to your data, you can first prep() the recipe to estimate the parameters needed for each step and then bake(new_data = NULL) to pull out the training data with those steps applied.

Now the recipe is ready to be applied to the test data.

```{r bake_test}
#ready for predictions at the end
baked_test <- bake(knn_rec, churn_test)
```

##Specify the k-nearest neighbor model

```{r knn_spec}
#tell it how many neighbors to look at to make a decision
knn_spec <- nearest_neighbor(neighbors = 5) %>% #set neighbors
  set_engine("kknn") %>%  #method of estimation (a distinction)
  set_mode("classification") 
```

```{r knn_fit}
knn_fit <- knn_spec %>% 
  fit(Attrition ~. , data = churn_train)
```

However better way to determine k in the model


```{r cv}
#this chunk is highly recommended to set.seed for reproducibility and to redo to get same answer
#we get same results if set.seed if in this chunk verses in the global environment. if set seed is in environment we get different answers (idk why)
set.seed(808)
# 5-fold CV on the training dataset (instead of 10 for in-class demo)

cv_folds <- churn_train %>% vfold_cv(5)

```

We now have a recipe for processing the data, a model specification, and CV splits for the training data.

Let's put it all together in a workflow.

```{r knn_workflow}

knn_workflow <- workflow() %>% 
  add_model(knn_spec) %>% 
  add_recipe(knn_rec)
  
```
Let's fit the resamples and carry out the cross-validation
```{r knn_res}
#fit our model to each of these folds
knn_res <- knn_workflow %>% 
  fit_resamples(
    resamples = cv_folds,
    control = control_resamples(save_pred = T)
  )
```

```{r}
# Check the performance
knn_res %>% collect_metrics()

#taking the performance from each run and averaging it (from each folds) that is what we see in the mean
#ask allie again about this
```

Let's find the best value of k
```{r spec_with_tuning}
# Define our KNN model with tuning
#going to specify the model again. Telling that we are going to tune that parameter to get the best model
knn_spec_tune <- nearest_neighbor(neighbors = tune()) %>% 
  set_mode("classification") %>% 
  set_engine("kknn")
  
#when we check this, we see that the neighbors is equal to tune()
```

```{r wf_knn_tune}
# Define a new workflow
wf_knn_tune <- workflow() %>% 
  add_model(knn_spec_tune) %>% 
  add_recipe(knn_rec) 
```

This time before we fit the model we need to tell R which values to try for the parameter that we're tuning.

To tune our hyperparameter(s), we will use the tune_grid() function (instead of the fit() or fit_resamples() functions).

This tune_grid() is similar to fit_resamples() except that it takes an additional argument: grid. We will pass the possible values of our hyperparameter(s) to this grid argument, and it will evaluate each fold of our sample on each set of hyperparameters passed to grid.

We'll explore a few values of k: (1,5,10, 20, 30, 40, 50, 60, 70, 80, 90, 100)
  - going to give k all these values and see which k is the best

```{r fit_knn_cv}
# Fit the workflow on our predefined folds and a grid of hyperparameters
fit_knn_cv <- wf_knn_tune %>% 
  tune_grid(
    cv_folds,
    grid = data_frame(neighbors = c(1,5,seq(10,100,10))) #in this case its just a list of numbers, telling it how much to try
  )

#model is runing 60 times because 5 folds and 12 k we want to try, we want to do a validation of each of those folds per k
# Check the performance with collect_metrics()
collect_metrics(fit_knn_cv)

#or

#fit_knn_cv %>% collect_metrics()
```

And finally, we will predict.

Use finalize_workflow() function wants (1) your initial workflow and (2) your best model.

```{r final_wf}
# The final workflow for our KNN model. Finalize_workflow takes a workflow and a set of parameters.  In this case, that set is just the best value of k
final_wf <- wf_knn_tune %>% 
  finalize_workflow(select_best(fit_knn_cv, metric = "accuracy"))

# Check out the final workflow object.  Choosing accuracy for interpretability in this simple binary context
final_wf

#in this case, our neighbor is 30, meaning 30 is our best model
```

```{r final_fit}
# Fitting our final workflow
final_fit <- final_wf %>% 
  fit(data = churn_train)

# Examine the final workflow
final_fit
```

And finally, we can predict onto the testing dataset.

```{r churn_pred}
#final_fit is our model (fit to the training data)
#workflow applied to the training data
churn_pred <- final_fit %>% 
  predict(new_data = churn_test)

#actual prediction of each point of the test data (values for the outcome table)
churn_pred %>% head()
```

There's a better way! You can pass your final workflow (workflow plus the best model) to the last_fit() function along with your initial split (for us: churn_split) to both (a) fit your final model on your full training dataset and (b) make predictions onto the testing dataset (defined in your initial split object).

This last_fit() approach streamlines your work (combining steps) and also lets you easily collect metrics using the collect_metrics() function

```{r last_fit}
# Write over 'final_fit' with this last_fit() approach
#basically doing it again in a more streamline way

final_fit <- final_wf %>% 
  last_fit(churn_split) #giving both the test and training data together
# Collect metrics on the test data!
final_fit %>% collect_metrics()

#these metrics apply to the testing data (for the last_fit() function)
#using last_fit taking workflow, it is fitting to the training and testing data both. and we get the answer for the testing data
```
